{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Scrapy框架爬取糗事百科段子："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用命令创建一个爬虫：\n",
    "   \n",
    "scrapy gensipder qsbk \"qiushibaike.com\"   \n",
    "   \n",
    "创建了一个名字叫做qsbk的爬虫，并且能爬取的网页只会限制在qiushibaike.com这个域名下。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 爬虫代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class QsbkSpider(scrapy.Spider):\n",
    "    name = 'qsbk'\n",
    "    allowed_domains = ['qiushibaike.com']\n",
    "    start_urls = ['http://qiushibaike.com/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬虫代码解析：\n",
    "   \n",
    "其实这些代码我们完全可以自己手动去写，而不用命令。只不过是不用命令，自己写这些代码比较麻烦。   \n",
    "要创建一个Spider，那么必须自定义一个类，继承自scrapy.Spider，然后在这个类中定义三个属性和一个方法。   \n",
    "   \n",
    "1. name：这个爬虫的名字，名字必须是唯一的。\n",
    "2. allow_domains：允许的域名。爬虫只会爬取这个域名下的网页，其他不是这个域名下的网页会被自动忽略。\n",
    "3. start_urls：爬虫从这个变量中的url开始。\n",
    "4. parse：引擎会把下载器下载回来的数据扔给爬虫解析，爬虫再把数据传给这个parse方法。这个是个固定的写法。这个方法的作用有两个，第一个是提取想要的数据。第二个是生成下一个请求的url。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
