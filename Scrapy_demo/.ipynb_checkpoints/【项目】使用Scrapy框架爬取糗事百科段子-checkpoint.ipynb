{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Scrapy框架爬取糗事百科段子："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用命令创建一个爬虫：\n",
    "   \n",
    "scrapy gensipder qsbk \"qiushibaike.com\"   \n",
    "   \n",
    "创建了一个名字叫做qsbk的爬虫，并且能爬取的网页只会限制在qiushibaike.com这个域名下。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 爬虫代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class QsbkSpider(scrapy.Spider):\n",
    "    name = 'qsbk'\n",
    "    allowed_domains = ['qiushibaike.com']\n",
    "    start_urls = ['http://qiushibaike.com/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬虫代码解析：\n",
    "   \n",
    "其实这些代码我们完全可以自己手动去写，而不用命令。只不过是不用命令，自己写这些代码比较麻烦。   \n",
    "要创建一个Spider，那么必须自定义一个类，继承自scrapy.Spider，然后在这个类中定义三个属性和一个方法。   \n",
    "   \n",
    "1. name：这个爬虫的名字，名字必须是唯一的。\n",
    "2. allow_domains：允许的域名。爬虫只会爬取这个域名下的网页，其他不是这个域名下的网页会被自动忽略。\n",
    "3. start_urls：爬虫从这个变量中的url开始。\n",
    "4. parse：引擎会把下载器下载回来的数据扔给爬虫解析，爬虫再把数据传给这个parse方法。这个是个固定的写法。这个方法的作用有两个，第一个是提取想要的数据。第二个是生成下一个请求的url。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 修改settings.py代码：\n",
    "\n",
    "在做一个爬虫之前，一定要记得修改setttings.py中的设置。两个地方是强烈建议设置的。\n",
    "\n",
    "1. ROBOTSTXT_OBEY设置为False。默认是True。即遵守机器协议，那么在爬虫的时候，scrapy首先去找robots.txt文件，如果没有找到。则直接停止爬取。\n",
    "2. DEFAULT_REQUEST_HEADERS添加User-Agent。这个也是告诉服务器，我这个请求是一个正常的请求，不是一个爬虫。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完成的爬虫代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1.爬虫部分代码：\n",
    "import scrapy\n",
    "from abcspider.items import QsbkItem\n",
    "\n",
    "class QsbkSpider(scrapy.Spider):\n",
    "    name = 'qsbk'\n",
    "    allowed_domains = ['qiushibaike.com']\n",
    "    start_urls = ['https://www.qiushibaike.com/text/']\n",
    "\n",
    "     def parse(self, response):\n",
    "         outerbox = response.xpath(\"//div[@id='content-left']/div\")\n",
    "         items = []\n",
    "         for box in outerbox:\n",
    "             author = box.xpath(\".//div[contains(@class,'author')]//h2/text()\").extract_first().strip()\n",
    "             content = box.xpath(\".//div[@class='content']/span/text()\").extract_first().strip()\n",
    "             item = QsbkItem()\n",
    "             item[\"author\"] = author\n",
    "             item[\"content\"] = content\n",
    "             items.append(item)\n",
    "         return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2.items.py部分代码：\n",
    "import scrapy\n",
    "class QsbkItem(scrapy.Item):\n",
    "    author = scrapy.Field()\n",
    "    content = scrapy.Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3.pipeline部分代码：\n",
    "import json\n",
    "\n",
    "class AbcspiderPipeline(object):\n",
    "    def __init__(self):\n",
    "        self.items = []\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        self.items.append(dict(item))\n",
    "        print(\"=\"*40)\n",
    "        return item\n",
    "    \n",
    "    def close_spider(self,spider):\n",
    "        with open('qsbk.json','w',encoding='utf-8') as fp:\n",
    "            json.dump(self.items,fp,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 运行scrapy项目：\n",
    "\n",
    "运行scrapy项目。需要在终端，进入项目所在的路径，然后scrapy crawl [爬虫名字]即可运行指定的爬虫。如果不想每次都在命令行中运行，那么可以把这个命令写在一个文件中。以后就在pycharm中执行运行这个文件就可以了。比如现在新创建一个文件叫做start.py，然后在这个文件中填入以下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scrapy import cmdline\n",
    "\n",
    "cmdline.execute(\"scrapy crawl qsbk\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 糗事百科爬虫笔记：\n",
    "<img src=\"qsbk_spider.jpg\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JsonItemExporter和JsonLinesItemExporter\n",
    "保存json数据到时候，可以使用这两个类，让操作变得更简单。（示例代码如下）\n",
    "1. 'JsonItemExporter'：这个是每次把数据添加到内存中，最后统一写入到硬盘中。\n",
    "   好处是，存储的数据是一个满足json规则的数据；坏处是如果数据量比较大，那么比较耗内存\n",
    "   \n",
    "2. 'JsonLinesItemExporter'：这个是每次调用'export_item'的时候就把这个item存储到硬盘中。\n",
    "   坏处是每一个字典是一行，整个文件不是一个满足json格式的文件；好处是每次处理数据到时候就直接存储到硬盘中，这样不会耗内存，数据比较安全"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 示例1：JsonItemExporter\n",
    "from scrapy.exporters import JsonItemExporter\n",
    "\n",
    "class QsbkPipeline(object):\n",
    "    def __init__(self):\n",
    "        self.fp = open(\"duanzi.json\", \"wb\")   #以二进制的方式写入\n",
    "        self.exportr = JsonItemExporter(self.fp, ensure_ascii=False, encoding='utf-8')\n",
    "        self.exportr.start_exporting()\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        print('爬虫开始了……')\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        self.exportr.export_item(item)\n",
    "        return item\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.exportr.finish_exporting()\n",
    "        self.fp.close()\n",
    "        print('爬虫结束了…')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 示例2：JsonLinesItemExporter\n",
    "from scrapy.exporters import JsonLinesItemExporter\n",
    "\n",
    "class QsbkPipeline(object):\n",
    "    def __init__(self):\n",
    "        self.fp = open(\"duanzi.json\", \"wb\")   #以二进制的方式写入\n",
    "        self.exportr = JsonLinesItemExporter(self.fp, ensure_ascii=False, encoding='utf-8')\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        print('爬虫开始了……')\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        self.exportr.export_item(item)\n",
    "        return item\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.fp.close()\n",
    "        print('爬虫结束了…')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
