{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrawlSpider\n",
    "\n",
    "在上一个糗事百科的爬虫案例中。我们是自己在解析完整个页面后获取下一页的url，然后重新发送一个请求。有时候我们想要这样做，只要满足某个条件的url，都给我进行爬取。那么这时候我们就可以通过CrawlSpider来帮我们完成了。CrawlSpider继承自Spider，只不过是在之前的基础之上增加了新的功能，可以定义爬取的url的规则，以后scrapy碰到满足条件的url都进行爬取，而不用手动的yield Request。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrawlSpider爬虫："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建CrawlSpider爬虫：\n",
    "   \n",
    "之前创建爬虫的方式是通过scrapy genspider [爬虫名字] [域名]的方式创建的。如果想要创建CrawlSpider爬虫，那么应该通过以下命令创建：   \n",
    "   \n",
    "scrapy genspider -c crawl [爬虫名字] [域名]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinkExtractors链接提取器：\n",
    "使用LinkExtractors可以不用程序员自己提取想要的url，然后发送请求。这些工作都可以交给LinkExtractors，他会在所有爬的页面中找到满足规则的url，实现自动的爬取。以下对LinkExtractors类做一个简单的介绍："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class scrapy.linkextractors.LinkExtractor(\n",
    "    allow = (),\n",
    "    deny = (),\n",
    "    allow_domains = (),\n",
    "    deny_domains = (),\n",
    "    deny_extensions = None,\n",
    "    restrict_xpaths = (),\n",
    "    tags = ('a','area'),\n",
    "    attrs = ('href'),\n",
    "    canonicalize = True,\n",
    "    unique = True,\n",
    "    process_value = None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 主要参数讲解：\n",
    "   \n",
    "1. allow：允许的url。所有满足这个正则表达式的url都会被提取。\n",
    "2. deny：禁止的url。所有满足这个正则表达式的url都不会被提取。\n",
    "3. allow_domains：允许的域名。只有在这个里面指定的域名的url才会被提取。\n",
    "4. deny_domains：禁止的域名。所有在这个里面指定的域名的url都不会被提取。\n",
    "5. restrict_xpaths：严格的xpath。和allow共同过滤链接。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule规则类：\n",
    "定义爬虫的规则类。以下对这个类做一个简单的介绍："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class scrapy.spiders.Rule(\n",
    "    link_extractor, \n",
    "    callback = None, \n",
    "    cb_kwargs = None, \n",
    "    follow = None, \n",
    "    process_links = None, \n",
    "    process_request = None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 主要参数讲解：\n",
    "   \n",
    "1. link_extractor：一个LinkExtractor对象，用于定义爬取规则。\n",
    "2. callback：满足这个规则的url，应该要执行哪个回调函数。因为CrawlSpider使用了parse作为回调函数，因此不要覆盖parse作为回调函数自己的回调函数。\n",
    "3. follow：指定根据该规则从response中提取的链接是否需要跟进。\n",
    "4. process_links：从link_extractor中获取到链接后会传递给这个函数，用来过滤不需要爬取的链接。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 微信小程序社区CrawlSpider案例"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
